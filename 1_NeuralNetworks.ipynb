{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_NeuralNetworks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/21400126/Ionic/blob/master/1_NeuralNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5mSO64ga8Ez",
        "colab_type": "text"
      },
      "source": [
        "nn.Module : contains layers\n",
        "\n",
        "forward(input) : returns the output\n",
        "\n",
        "Training procedure\n",
        "Define the neural network that has some learnable parameters (or weights) -> Iterate over a dataset of inputs -> Process input through the network\n",
        "-> Compute the loss (how far is the output from being correct) -> Propagate gradients back into the network’s parameters \n",
        "->Update the weights of the network, typically using a simple update rule: weight = weight - learning_rate * gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v55ipBCvbDSA",
        "colab_type": "code",
        "outputId": "2fdf4b5a-bc17-46a2-f587-b2484c8f3664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "#Define the network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F    #Neural Network 만들때 필요함\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "    # nn.Conv2d will take in a 4D tensor of \"nSamples * nChannels * height * width\"\n",
        "    self.conv1 = nn.Conv2d(1,6,3)       # 1input image channel, 6 output channels, 3x3 square convolution  \n",
        "    self.conv2 = nn.Conv2d(6,16,3)\n",
        "    \n",
        "    self.fc1 = nn.Linear(16*6*6,120)       #6*6 from image dimension // input_dim, output_dim // 6*6은 무엇인가...\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    # Max pooling over a (2,2) window\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))       #???\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)),2)        # size가 squeare라면 single number 사용 가능 => out_dim이 16이라서 square?\n",
        "    \n",
        "    x = x.view(-1,self.num_flat_features(x)) \n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "  \n",
        "  def num_flat_features(self, x): # 하나의 열벡터로 만들어준다\n",
        "    size = x.size()[1:]   \n",
        "    num_features = 1\n",
        "    for s in size :\n",
        "      num_features *= s                     #?\n",
        "    return num_features\n",
        "    \n",
        "net = Net()\n",
        "print(net)\n",
        "\n",
        "params = list(net.parameters())            # 밑으로 이해 불가\n",
        "print(len(params))\n",
        "print(params[0].size())                     # 파라미터 자신들이 만들어줌\n",
        "\n",
        "input = torch.randn(1,1,32,32)              # input \n",
        "out = net(input)                              \n",
        "print(out)\n",
        "\n",
        "'''\n",
        "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
        "      -> view -> linear -> relu -> linear -> relu -> linear\n",
        "      -> MSELoss\n",
        "      -> loss\n",
        "'''\n",
        "\n",
        "net.zero_grad() # back / forward할 시 gradient가 0가 되야 안정적이라고 한다. 밑에서 backward할 거니 미리 zero_grad로 만들자\n",
        "out.backward(torch.randn(1,10)) #backward를 통해 학습을 진행\n",
        "\n",
        "#Loss Function\n",
        "output = net(input)\n",
        "target = torch.randn(10)\n",
        "target = target.view(1,-1)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)  #loss가 몇인 지 알아서 구해줌\n",
        "print(loss)\n",
        "\n",
        "print(loss.grad_fn) #MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0]) #Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) #ReLu\n",
        "\n",
        "#Backprop\n",
        "net.zero_grad() \n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()             #back propagation하면서 loss구해주고 training도 진행되는 과정\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "# Update the weights\n",
        "\n",
        "'''   Simple python code\n",
        "learning_rate = 0.01\n",
        "for f in net.parameters():\n",
        "  f.data.sub_(f.grad.date * learnig_rate)\n",
        " '''\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)  # create optimizer\n",
        "\n",
        "# in training loop\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step() # Update, gradient을 각 parameter에 더해준다."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "10\n",
            "torch.Size([6, 1, 3, 3])\n",
            "tensor([[ 0.0201, -0.0864,  0.0156, -0.0673, -0.0786, -0.0141, -0.0091,  0.0640,\n",
            "         -0.0345, -0.0115]], grad_fn=<AddmmBackward>)\n",
            "tensor(0.8450, grad_fn=<MseLossBackward>)\n",
            "<MseLossBackward object at 0x7f678a9b6c18>\n",
            "<AddmmBackward object at 0x7f678a9b6dd8>\n",
            "<AccumulateGrad object at 0x7f678a8f6fd0>\n",
            "conv1.bias.grad before backward\n",
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "conv1.bias.grad after backward\n",
            "tensor([ 0.0036, -0.0081, -0.0060,  0.0121, -0.0029,  0.0072])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}