{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_Autograd.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/21400126/Ionic/blob/master/1_Autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clun5HcLa5gx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "264eb1c3-351e-47be-be08-83faede3fcab"
      },
      "source": [
        "'''\n",
        "autograd : package which contains central to all neural networks in Pytorch\n",
        "\n",
        "Feature \n",
        "1. providde automatic differentiation for all operations on Tensors\n",
        "         => it means backprop is defined by how my code is run, and that every single iteration can be different\n",
        "\n",
        "torch.Tensor => central class of package\n",
        "\n",
        ".requires_grad : if I set its attribute True , it starts to track all operations on it\n",
        "\n",
        ".backward() : when I finish my computation, then have all the gradients computed automatically , if want to compute the derivatives\n",
        "\n",
        ".grad : The gradient for this tensor will be accumulated into this attribute\n",
        "\n",
        ".detach() : to stop a tensor from tracking history / to detach it from the computation history, and to prevent future computation\n",
        "\n",
        "torch.no_grad() : to prevent tracking history, using memory, i can also wrap the code block  // when evaluating a model it's so helpful\n",
        "\n",
        "Function\n",
        "\n",
        "torch.autograd = engine for computing vector-Jacobian product\n",
        "* Each tensor has a \".grad_fn\" \n",
        "'''\n",
        "\n",
        "import torch\n",
        "'''\n",
        "x = torch.ones(2,2, requires_grad = True)\n",
        "print(x)  \n",
        "\n",
        "y = x+2\n",
        "print(y)                  #grad_fn =<AddBackward0> 라는 값도 같이 출력됨\n",
        "\n",
        "print(y.grad_fn)       # operation의 결과로써 생겼기 때문에, grad_fn을 가진다 => grad_fn이 뭔데?\n",
        "\n",
        "\n",
        "z= y * y * 3\n",
        "out = z.mean()           # what is z.mean()? => \n",
        "\n",
        "print(z)\n",
        "print(out)                 # contains single scalar => why? // grad_fn=<MeanBackward0>라고 뜸 \n",
        " \n",
        "a=torch.randn(2,2)\n",
        "print(\"a=\",a)\n",
        "\n",
        "a = ((a*3)/(a-1))\n",
        "print(a.requires_grad)#False\n",
        "\n",
        "a.requires_grad_(True)\n",
        "print(a.requires_grad)#True\n",
        "\n",
        "print (a)\n",
        "\n",
        "b=(a*a).sum()        # what's it meaning?\n",
        "print(b)       # too             <SumBackward0>\n",
        "\n",
        "#out.backward() == out.backward(torch.tensor(1.))  YYYY?\n",
        "\n",
        "out.backward()           #?\n",
        "print(x.grad)             #\n",
        "'''\n",
        "\n",
        "x = torch.randn(3, requires_grad = True)\n",
        "\n",
        "y= x*2\n",
        "\n",
        "while y.data.norm() < 1000:\n",
        "  y = y * 2\n",
        "  \n",
        "print(y) # y is no longer a scalar. but idk y?\n",
        "\n",
        "v = torch.tensor([0.1,1.0,0.0001],dtype=torch.float)\n",
        "print(x.grad) #None\n",
        "\n",
        "y.backward(v)\n",
        "print(x.grad) #y.backward(v)로 왜 x.grad의 값이 나오는지 모르겠음\n",
        "\n",
        "print(x.requires_grad)\n",
        "print((x**2).requires_grad)\n",
        "\n",
        "with torch.no_grad() : \n",
        "  print((x**2).requires_grad)\n",
        "  \n",
        "with torch.no_grad() :  \n",
        "  print(x.requires_grad)         # 이건 왜 True?\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([972.0233, 827.3616, -65.4969], grad_fn=<MulBackward0>)\n",
            "None\n",
            "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n",
            "True\n",
            "True\n",
            "True\n",
            "False\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}