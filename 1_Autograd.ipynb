{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_Autograd.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/21400126/Pytorch/blob/master/1_Autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clun5HcLa5gx",
        "colab_type": "code",
        "outputId": "334f1c7e-f7c7-4315-9c29-b8f3bb2589aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        }
      },
      "source": [
        "'''\n",
        "autograd : 자동미분을 수행하는 토치의 핵심 패키지\n",
        "forward 방향으로 수행하는 모든 연산을 기억, backward로 연산들을 재생\n",
        "\n",
        ".requires_grad : if I set its attribute True , it starts to track all operations on it => True속성을 가지고 있다면 계산은 추적된다\n",
        "\n",
        ".backward() : when I finish my computation, then have all the gradients computed automatically , if want to compute the derivatives\n",
        "스칼라(한개의 element를 가지고 있는) 형태 => 파라미터 필요 x / otherwise, 텐서의 matching shape를 gradient의 인자로 지정해야한다\n",
        "\n",
        ".grad : The gradient for this tensor will be accumulated into this attribute\n",
        "=> backward() 계산한 후, 변수에 대한 그라디언트는 이 속성에 누적된다.\n",
        "\n",
        ".detach() : to stop a tensor from tracking history / to detach it from the computation history, and to prevent future computation\n",
        "\n",
        "torch.no_grad() : to prevent tracking history, using memory, i can also wrap the code block  // when evaluating a model it's so helpful\n",
        "\n",
        "Function => 각 변수는 Tensor를 생성한 Function을 참조하는 .grad_fn속성을 가지고 있다\n",
        "\n",
        "requires_grad_ 는 이미 존재하는 텐서의 requires_grad플래그를 내부에서 변경함 / 입력 플래그 명시하지 않은 경우 기본적으로 True\n",
        "torch.autograd = engine for computing vector-Jacobian product\n",
        "'''\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.ones(2,2, requires_grad = True)\n",
        "print(x)  \n",
        "\n",
        "y = x+2\n",
        "print(y)                  #grad_fn =<AddBackward0> 라는 값도 같이 출력됨\n",
        "\n",
        "print(y.grad_fn)       # operation의 결과로써 생겼기 때문에, grad_fn을 가진다\n",
        "\n",
        "\n",
        "z= y * y * 3\n",
        "out = z.mean()           # what is z.mean()? => to average whole tensor\n",
        "\n",
        "print(z)\n",
        "print(out)                 # contains single scalar => why? => .mean()을 통해 scalar값으로 만들어 주었음\n",
        " \n",
        "a=torch.randn(2,2)\n",
        "print(\"a=\",a)\n",
        "\n",
        "a = ((a*3)/(a-1))\n",
        "print(a.requires_grad)#False\n",
        "\n",
        "a.requires_grad_(True)\n",
        "print(a.requires_grad)#True\n",
        "\n",
        "print (a)\n",
        "\n",
        "b=(a*a).sum()       \n",
        "print(b)      \n",
        "\n",
        "#out.backward() == out.backward(torch.tensor(1.))\n",
        "\n",
        "out.backward()           # 모든 값이 4.5인 2x2 행렬\n",
        "print(x.grad)             \n",
        "\n",
        "\n",
        "x = torch.randn(3, requires_grad = True)\n",
        "\n",
        "y= x*2\n",
        "\n",
        "while y.data.norm() < 1000:\n",
        "  y = y * 2\n",
        "  \n",
        "print(y) \n",
        "\n",
        "v = torch.tensor([0.1,1.0,0.0001],dtype=torch.float)\n",
        "print(x.grad) #None\n",
        "\n",
        "\n",
        "x.backward(v)\n",
        "print(x.grad)# 이 값 왜 v인지 몰겠음\n",
        "\n",
        "y.backward(v)\n",
        "print(x.grad) \n",
        "\n",
        "print(x.requires_grad)             #True\n",
        "print((x**2).requires_grad)        #True\n",
        "\n",
        "with torch.no_grad() :  # 이것으로 래핑하여 requires_grad = True속성이 명시된 autograd가 텐서의 추적기록에 남지 않게 한다\n",
        "  print((x**2).requires_grad)         #False\n",
        "  \n",
        "with torch.no_grad() :  \n",
        "  print(x.requires_grad)         # 이건 왜 True? => 아무 연산도 진행되지 않은 것은 True를 default 값으로 한다\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n",
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n",
            "<AddBackward0 object at 0x7f24399f2550>\n",
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>)\n",
            "tensor(27., grad_fn=<MeanBackward0>)\n",
            "a= tensor([[ 0.7123, -0.3707],\n",
            "        [-1.9919, -1.6206]])\n",
            "False\n",
            "True\n",
            "tensor([[-7.4273,  0.8113],\n",
            "        [ 1.9973,  1.8552]], requires_grad=True)\n",
            "tensor(63.2536, grad_fn=<SumBackward0>)\n",
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n",
            "tensor([1025.0343,  128.9029, -319.9389], grad_fn=<MulBackward0>)\n",
            "None\n",
            "tensor([1.0000e-01, 1.0000e+00, 1.0000e-04])\n",
            "tensor([5.1300e+01, 5.1300e+02, 5.1300e-02])\n",
            "True\n",
            "True\n",
            "False\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}